{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61f2b345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras import Sequential, Input, layers\n",
    "import tensorflow_datasets as tfds\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1121d73",
   "metadata": {},
   "source": [
    "# (3) Custom embedding with layers.Embedding\n",
    "\n",
    "Find an embedding that is specifically designed for your task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac9c6ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16 different words in your corpus\n",
      "X_pad.shape (3, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  3.,  4.,  6.,  0.,  0.,  0.],\n",
       "       [ 1.,  2.,  7.,  4.,  8.,  9., 10., 11.],\n",
       "       [12.,  3.,  5., 13., 14., 15.,  5., 16.]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our RNN input will look like this tensor\n",
    "# X.shape = (n_sentences, max_sentence_length, embedding_dim)\n",
    "\n",
    "# Let's create some mock dat\n",
    "def get_mock_up_data():\n",
    "    sentence_1 = 'Deep learning is super easy'\n",
    "    sentence_2 = 'Deep learning was super bad and too long'\n",
    "    sentence_3 = 'This is the best lecture of the camp!'\n",
    "\n",
    "    X = [sentence_1, sentence_2, sentence_3]\n",
    "    y = np.array([1., 0., 0.])\n",
    "\n",
    "    # Let's tokenize the vocabulary\n",
    "    tk = Tokenizer()\n",
    "    tk.fit_on_texts(X)\n",
    "    vocab_size = len(tk.word_index)\n",
    "    \n",
    "    print(f'There are {vocab_size} different words in your corpus')\n",
    "    X_token = tk.texts_to_sequences(X)\n",
    "\n",
    "    # Pad the inputs\n",
    "    X_pad = pad_sequences(X_token, dtype='float32', padding='post')\n",
    "\n",
    "    return X_pad, y, vocab_size\n",
    "\n",
    "X_pad, y, vocab_size = get_mock_up_data()\n",
    "print(\"X_pad.shape\", X_pad.shape)\n",
    "X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58ea872a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,700</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,680</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m100\u001b[0m)         │         \u001b[38;5;34m1,700\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │         \u001b[38;5;34m9,680\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m21\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,401</span> (44.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,401\u001b[0m (44.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,401</span> (44.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,401\u001b[0m (44.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Embedding(input_dim=VOCAB_SIZE, input_length=MAX_SENTENCE_LENGTH, output_dim=EMBED_DIM, mask_zero=True)\n",
    "\n",
    "# Size of your embedding space = size of the vector representing each word\n",
    "embedding_size = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=X_pad.shape[1:]))\n",
    "model.add(layers.Embedding(\n",
    "    input_dim=vocab_size+1, # 16 + 1 for the 0 padding\n",
    "    output_dim=embedding_size,\n",
    "    mask_zero=True # Built-in masking layer\n",
    "))\n",
    "\n",
    "model.add(layers.LSTM(20))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "243ecdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected number of parameters in Embedding: 1700\n"
     ]
    }
   ],
   "source": [
    "print(f'Expected number of parameters in Embedding: {(vocab_size+1) * embedding_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "979eab06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2223a74f860>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.fit(X_pad, y, epochs=5, batch_size=16, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fc74d8",
   "metadata": {},
   "source": [
    "# (4.2) Word2vec: Implementation with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8159ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\luan.barbosa\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1079: The name tf.data.get_output_shapes is deprecated. Please use tf.compat.v1.data.get_output_shapes instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\luan.barbosa\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1079: The name tf.data.get_output_shapes is deprecated. Please use tf.compat.v1.data.get_output_shapes instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\",\n",
       "       b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's get some text first\n",
    "train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True)\n",
    "\n",
    "train_sentences, train_labels = tfds.as_numpy(train_data)\n",
    "test_sentences, test_labels = tfds.as_numpy(test_data)\n",
    "\n",
    "# Let's check two sentences\n",
    "train_sentences[0:2]\n",
    "\n",
    "# We have to convert the sentences into list of words! The computer won't do it for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5a6b527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this',\n",
       "  'was',\n",
       "  'an',\n",
       "  'absolutely',\n",
       "  'terrible',\n",
       "  'movie',\n",
       "  \"don't\",\n",
       "  'be',\n",
       "  'lured',\n",
       "  'in',\n",
       "  'by',\n",
       "  'christopher',\n",
       "  'walken',\n",
       "  'or',\n",
       "  'michael',\n",
       "  'ironside',\n",
       "  'both',\n",
       "  'are',\n",
       "  'great',\n",
       "  'actors',\n",
       "  'but',\n",
       "  'this',\n",
       "  'must',\n",
       "  'simply',\n",
       "  'be',\n",
       "  'their',\n",
       "  'worst',\n",
       "  'role',\n",
       "  'in',\n",
       "  'history',\n",
       "  'even',\n",
       "  'their',\n",
       "  'great',\n",
       "  'acting',\n",
       "  'could',\n",
       "  'not',\n",
       "  'redeem',\n",
       "  'this',\n",
       "  \"movie's\",\n",
       "  'ridiculous',\n",
       "  'storyline',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'is',\n",
       "  'an',\n",
       "  'early',\n",
       "  'nineties',\n",
       "  'us',\n",
       "  'propaganda',\n",
       "  'piece',\n",
       "  'the',\n",
       "  'most',\n",
       "  'pathetic',\n",
       "  'scenes',\n",
       "  'were',\n",
       "  'those',\n",
       "  'when',\n",
       "  'the',\n",
       "  'columbian',\n",
       "  'rebels',\n",
       "  'were',\n",
       "  'making',\n",
       "  'their',\n",
       "  'cases',\n",
       "  'for',\n",
       "  'revolutions',\n",
       "  'maria',\n",
       "  'conchita',\n",
       "  'alonso',\n",
       "  'appeared',\n",
       "  'phony',\n",
       "  'and',\n",
       "  'her',\n",
       "  'pseudo',\n",
       "  'love',\n",
       "  'affair',\n",
       "  'with',\n",
       "  'walken',\n",
       "  'was',\n",
       "  'nothing',\n",
       "  'but',\n",
       "  'a',\n",
       "  'pathetic',\n",
       "  'emotional',\n",
       "  'plug',\n",
       "  'in',\n",
       "  'a',\n",
       "  'movie',\n",
       "  'that',\n",
       "  'was',\n",
       "  'devoid',\n",
       "  'of',\n",
       "  'any',\n",
       "  'real',\n",
       "  'meaning',\n",
       "  'i',\n",
       "  'am',\n",
       "  'disappointed',\n",
       "  'that',\n",
       "  'there',\n",
       "  'are',\n",
       "  'movies',\n",
       "  'like',\n",
       "  'this',\n",
       "  'ruining',\n",
       "  \"actor's\",\n",
       "  'like',\n",
       "  'christopher',\n",
       "  \"walken's\",\n",
       "  'good',\n",
       "  'name',\n",
       "  'i',\n",
       "  'could',\n",
       "  'barely',\n",
       "  'sit',\n",
       "  'through',\n",
       "  'it'],\n",
       " ['i',\n",
       "  'have',\n",
       "  'been',\n",
       "  'known',\n",
       "  'to',\n",
       "  'fall',\n",
       "  'asleep',\n",
       "  'during',\n",
       "  'films',\n",
       "  'but',\n",
       "  'this',\n",
       "  'is',\n",
       "  'usually',\n",
       "  'due',\n",
       "  'to',\n",
       "  'a',\n",
       "  'combination',\n",
       "  'of',\n",
       "  'things',\n",
       "  'including',\n",
       "  'really',\n",
       "  'tired',\n",
       "  'being',\n",
       "  'warm',\n",
       "  'and',\n",
       "  'comfortable',\n",
       "  'on',\n",
       "  'the',\n",
       "  'sette',\n",
       "  'and',\n",
       "  'having',\n",
       "  'just',\n",
       "  'eaten',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'however',\n",
       "  'on',\n",
       "  'this',\n",
       "  'occasion',\n",
       "  'i',\n",
       "  'fell',\n",
       "  'asleep',\n",
       "  'because',\n",
       "  'the',\n",
       "  'film',\n",
       "  'was',\n",
       "  'rubbish',\n",
       "  'the',\n",
       "  'plot',\n",
       "  'development',\n",
       "  'was',\n",
       "  'constant',\n",
       "  'constantly',\n",
       "  'slow',\n",
       "  'and',\n",
       "  'boring',\n",
       "  'things',\n",
       "  'seemed',\n",
       "  'to',\n",
       "  'happen',\n",
       "  'but',\n",
       "  'with',\n",
       "  'no',\n",
       "  'explanation',\n",
       "  'of',\n",
       "  'what',\n",
       "  'was',\n",
       "  'causing',\n",
       "  'them',\n",
       "  'or',\n",
       "  'why',\n",
       "  'i',\n",
       "  'admit',\n",
       "  'i',\n",
       "  'may',\n",
       "  'have',\n",
       "  'missed',\n",
       "  'part',\n",
       "  'of',\n",
       "  'the',\n",
       "  'film',\n",
       "  'but',\n",
       "  'i',\n",
       "  'watched',\n",
       "  'the',\n",
       "  'majority',\n",
       "  'of',\n",
       "  'it',\n",
       "  'and',\n",
       "  'everything',\n",
       "  'just',\n",
       "  'seemed',\n",
       "  'to',\n",
       "  'happen',\n",
       "  'of',\n",
       "  'its',\n",
       "  'own',\n",
       "  'accord',\n",
       "  'without',\n",
       "  'any',\n",
       "  'real',\n",
       "  'concern',\n",
       "  'for',\n",
       "  'anything',\n",
       "  'else',\n",
       "  'i',\n",
       "  'cant',\n",
       "  'recommend',\n",
       "  'this',\n",
       "  'film',\n",
       "  'at',\n",
       "  'all']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's convert the list of sentences to a list of lists of words with a Keras utility function\n",
    "X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n",
    "X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n",
    "\n",
    "X_train[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63fa4074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.12063502,  0.03280268,  0.60021216, -0.41862488,  0.06843155,\n",
       "        0.3019776 , -0.02993482, -0.5249786 , -1.3902066 , -1.1858629 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This line trains an entire embedding for the words in your train set\n",
    "word2vec = Word2Vec(sentences=X_train, vector_size=10)\n",
    "\n",
    "# Let's take a look at the representation of any word\n",
    "word2vec.wv['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daf4d57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 0.9694231748580933),\n",
       " ('one', 0.9251149892807007),\n",
       " ('thing', 0.9249079823493958),\n",
       " ('sequel', 0.9088228344917297),\n",
       " ('still', 0.9034302830696106),\n",
       " ('it', 0.8977356553077698),\n",
       " ('comment', 0.8869630694389343),\n",
       " ('word', 0.8842864632606506),\n",
       " ('fun', 0.8837607502937317),\n",
       " ('effort', 0.8834584355354309)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's look at the 10 closest words to `movie`\n",
    "word2vec.wv.most_similar('movie', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47b29248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To control the size of the embedding space, use the `vector_size` keyword\n",
    "\n",
    "# We keep the training short by taking only 1000 sentences\n",
    "word2vec = Word2Vec(sentences=X_train[:1000], vector_size=50)\n",
    "\n",
    "len(word2vec.wv['computer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88b8ac2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word seen less than 5 times, and is thus excluded from corpus\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec learns a representation for words that are present more than `min_count` times\n",
    "# This is to prevent learning representations based on a few examples only\n",
    "\n",
    "word2vec = Word2Vec(sentences=X_train[:1000], vector_size=50, min_count=5)\n",
    "\n",
    "try:\n",
    "    len(word2vec.wv['columbian'])\n",
    "except:\n",
    "    print(\"Word seen less than 5 times, and is thus excluded from corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e17cd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As mentioned earlier, Word2Vec trains an internal neural network.\n",
    "# The goal of this network is to predict a word in a corpus based on the words around it. \n",
    "# This part of the sentence is called the window.\n",
    "# The window size is the number of words around word W used to predict word W.\n",
    "\n",
    "word2vec = Word2Vec(sentences=X_train[:10000], vector_size=100, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1474f942",
   "metadata": {},
   "source": [
    "## Pre-trained Word2Vec (transfer learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96d6d98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n",
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# Instead of training it on your training set (especially if it is very small), \n",
    "# you can directly load a pretrained embedding\n",
    "\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "\n",
    "model_wiki = gensim.downloader.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bbf4de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movies', 0.9322481155395508),\n",
       " ('film', 0.9310100078582764),\n",
       " ('films', 0.8937394618988037),\n",
       " ('comedy', 0.8902585506439209),\n",
       " ('hollywood', 0.8718216419219971),\n",
       " ('drama', 0.8341657519340515),\n",
       " ('sequel', 0.8222616314888),\n",
       " ('animated', 0.8216581344604492),\n",
       " ('remake', 0.812495768070221),\n",
       " ('show', 0.8105834126472473)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wiki.most_similar('movie', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3edd43",
   "metadata": {},
   "source": [
    "## Arithmetic on words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23fb1afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.9082302451133728),\n",
       " ('girl', 0.9080888032913208),\n",
       " ('man', 0.8935427665710449),\n",
       " ('guy', 0.855695903301239),\n",
       " ('cop', 0.8050817251205444),\n",
       " ('boy', 0.7836461663246155),\n",
       " ('doctor', 0.7555070519447327),\n",
       " ('town', 0.7552664875984192),\n",
       " ('lady', 0.7375656366348267),\n",
       " ('victim', 0.6935808062553406)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec = Word2Vec(sentences=X_train[:10000], vector_size=30, window=2, min_count=10)\n",
    "\n",
    "v_queen = word2vec.wv['queen']\n",
    "v_king = word2vec.wv['king']\n",
    "v_man = word2vec.wv['man']\n",
    "\n",
    "v_result = v_queen - v_king + v_man\n",
    "\n",
    "# Arithmetic directly on words\n",
    "word2vec.wv.similar_by_vector(v_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f770278",
   "metadata": {},
   "source": [
    "# (5) CNNs for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98d753cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luan.barbosa\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\layer.py:970: UserWarning: Layer 'conv1d_3' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">150,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_7 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m30\u001b[0m)          │       \u001b[38;5;34m150,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │         \u001b[38;5;34m4,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m21\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">154,101</span> (601.96 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m154,101\u001b[0m (601.96 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">154,101</span> (601.96 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m154,101\u001b[0m (601.96 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">150,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,820</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">121</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_8 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m30\u001b[0m)          │       \u001b[38;5;34m150,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m20\u001b[0m)          │         \u001b[38;5;34m1,820\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m121\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">151,941</span> (593.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m151,941\u001b[0m (593.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">151,941</span> (593.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m151,941\u001b[0m (593.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# 1D convolutions is the answer\n",
    "# Convoluções unidimensionais (layers.Conv1D(...)) fazem exatamente o que você quer: são\n",
    "# convoluções que \"deslizam\" ao longo do eixo da palavra, palavra por palavra.\n",
    "\n",
    "# RNN\n",
    "rnn = Sequential([\n",
    "    Input(shape=X_pad.shape[1:]),\n",
    "    layers.Embedding(input_dim=5000, output_dim=30, mask_zero=True),\n",
    "    layers.LSTM(20),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Conv1D\n",
    "cnn = Sequential ([\n",
    "    Input(shape=X_pad.shape[1:]),\n",
    "    layers.Embedding(input_dim=5000, output_dim=30, mask_zero=True),\n",
    "    layers.Conv1D(20, kernel_size=3),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "print(rnn.summary())\n",
    "print(cnn.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
